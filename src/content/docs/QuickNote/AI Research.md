---
title: AI Research
description: A detailed document about making of detailed IETM application using AR/VR technology
---
# **Weekly Report ‚Äì Er. Varun Sharma**

**Internship Start Date:** 10th October 2025  
**Project Focus:** Enhancing an existing IETM (Interactive Electronic Technical Manual) application using AI and AR/VR technologies.

---

## **Week 1 Summary**

### **1. Memento: Fine-tuning LLM Agents without Fine-tuning LLMs**

**Paper:** [Memento Research Paper](https://arxiv.org/pdf/2508.16153)  
**GitHub Repository:** [Agent-on-the-Fly/Memento](https://github.com/Agent-on-the-Fly/Memento)

**Overview:**  
Memento introduces a novel method of improving LLM agents _without fine-tuning the base LLM_. Instead, it enables adaptive learning through memory-based retrieval, reinforcement learning, and case-based reasoning. The LLM remains frozen while the agent evolves through experience.

#### ‚úÖ **Pros:**

- **No fine-tuning required:** Reduces computational cost and simplifies deployment across different LLM providers.
    
- **Continual learning capability:** Supports adaptive improvement via memory rewriting and retention without modifying model weights.
    
- **Strong benchmark performance:** Shows high accuracy on demanding benchmarks like **GAIA** and **DeepResearcher**.
    
- **Transparent learning process:** Uses a case-based memory system that can be audited, edited, or reset by developers.
    
- **Practical tooling ecosystem:** Includes web search, crawling, multimedia processing, and document parsing tools in its open-source repository.
    

#### ‚ö†Ô∏è **Cons:**

- **Complex memory management:** Requires effective handling of memory growth, retrieval accuracy, and pruning to avoid irrelevant or misleading recalls.
    
- **Limited capability growth:** Without weight updates, the agent may struggle on tasks requiring new reasoning skills rather than memory reuse.
    
- **Benchmark-specific optimization:** Most improvements are shown on deep research tasks‚Äîgeneralizability to other domains (e.g., robotics, AR/VR) is unclear.
    
- **Infrastructure overhead:** Requires systems for browsing, sandboxed execution, storage, and knowledge retrieval.
    
- **Opaque metrics:** Reported performance results lack detailed reproducible numbers in some cases, requiring further validation through leaderboards.
    

---

### **2. DeerFlow by ByteDance**

**GitHub Repository:** [Bytedance/deer-flow](https://github.com/bytedance/deer-flow)

**Overview:**  
DeerFlow is a community-driven deep research framework developed by ByteDance. It combines LLMs with tools like web search, crawling, Python execution, and multi-agent architectures to automate research workflows end-to-end. The system decomposes user queries, gathers and analyzes data, executes code, and generates reports or presentations.

#### ‚úÖ **Pros:**

1. **End-to-end research automation:** Supports complete workflows‚Äîfrom question understanding to report generation.
    
2. **Modular multi-agent architecture:** Components like Planner, Researcher, Coder, and Reporter make it extensible and maintainable.
    
3. **Integrated tool ecosystem:** Built-in access to web search, crawling, Python execution, document generation, and data analysis.
    
4. **Open-source and community-focused:** MIT-licensed with transparent code and active contributions.
    
5. **Multi-format outputs:** Generates reports, PowerPoint presentations, and even podcast-style audio summaries.
    
6. **Human-in-the-loop design:** Users can refine research stages, validate intermediate outputs, and ensure reliability.
    

#### ‚ö†Ô∏è **Cons:**

1. **High setup complexity:** Requires Python 3.12+, Node.js 22+, search/AI API keys, and system configurations, making onboarding difficult for beginners.
    
2. **Dependency on third-party services:** Relies on external APIs (Tavily, Brave Search, ArXiv, TTS) which may introduce latency, costs, or access restrictions.
    
3. **Risk of misinformation:** Automated summarization and code execution can produce incorrect or biased results without human supervision.
    
4. **Complex coordination between agents:** Multi-agent orchestration can create failure points or unpredictable behavior if not properly configured.
    
5. **Lack of standardized benchmarks:** Few peer-reviewed evaluations are available to measure performance against other research agents.
    
6. **Resource-intensive:** Requires strong local or cloud-based compute resources for crawling, API calls, and execution tasks.
    
7. **Narrow domain focus:** Optimized for research tasks, not real-time systems, robotics, or high-risk operational environments.
    

---

### **3. AutoGen & DeepResearcher**

#### **üîπ AutoGen (by Microsoft Research)**

**Overview:**  
AutoGen is an open-source framework for building **conversational multi-agent systems** using LLMs. It enables multiple AI agents‚Äîand even humans‚Äîto collaborate to solve complex tasks such as coding, research, debugging, and decision-making. Each agent can be given roles, goals, tools, and memory.

**‚úÖ Pros:**

- **Multi-agent collaboration:** Agents (e.g., Coder, Critic, Planner) can communicate autonomously to solve tasks.
    
- **Human-in-the-loop flexibility:** Users can intervene at any step or allow full automation.
    
- **Tool and API integration:** Agents can use Python execution, web search, file systems, and custom tools.
    
- **Highly customizable:** Developers can define workflows, agent personas, memory, and stopping rules.
    
- **Great for coding & reasoning tasks:** Strong performance in code generation, debugging, and mathematical reasoning.
    

**‚ö†Ô∏è Cons:**

- **Lacks built-in deep web research stack** (no native crawling, document extraction like DeerFlow).
    
- **Not optimized for real-world research outputs** like PDF reports, PPTs, or academic summaries.
    
- **Scalability issues** with many agents‚Äîcan become hard to manage or debug.
    
- **Higher token usage/costs** due to multi-agent dialogues.
    
- **Requires careful prompt & memory design** to avoid loops or irrelevant conversations.
    

---

#### **üîπ DeepResearcher (by Tencent)**

**Overview:**  
DeepResearcher is a reinforcement-learning based framework designed for **autonomous deep web research**. Unlike prompt-only agents, it **trains LLM agents using real web environments**‚Äîsearch engines, browsing, tool use‚Äîand optimizes them using reward signals.

**‚úÖ Pros:**

- **RL-based improvement:** Agents learn to plan, search, verify, and summarize information over time ‚Äî not just follow prompts.
    
- **Real web interaction:** Uses actual browsing/search APIs for up-to-date results.
    
- **Strong benchmark performance:** High scores on **GAIA, WebCPM, HotpotQA, and FEVER**.
    
- **Modular architecture:** Includes planner, browser, knowledge verifier, and summarizer agents.
    
- **Focus on truthful reasoning and citation-based answers.**
    

**‚ö†Ô∏è Cons:**

- **Heavy computational requirements:** Training needs GPUs, RL infrastructure, and simulated web environments.
    
- **Not fully plug-and-play:** More of a research framework than a ready-to-use product like DeerFlow.
    
- **Hard to reproduce experiments** due to dependency on specific environments and reward settings.
    
- **Still dependent on base LLM limitations** (hallucination, outdated knowledge if no web use).
    
- **Closed-source training details in some areas** (though inference pipeline is open).
    

---

### ‚úÖ **Comparison Table**

|Feature|**Memento**|**DeerFlow**|**AutoGen**|**DeepResearcher**|
|---|---|---|---|---|
|Agent Adaptation|Memory + RL (no LLM tuning)|Modular tool-based research|Multi-agent conversations|RL-trained web research|
|Fine-tuning Required?|‚ùå No|‚ùå No|‚ùå No|‚úÖ Yes (RL Finetuning)|
|Web Search & Crawling|‚úÖ Yes|‚úÖ Yes|‚ö†Ô∏è Limited (via plugins)|‚úÖ Native support|
|Code Execution|‚úÖ Yes|‚úÖ Yes|‚úÖ Yes|‚úÖ Yes|
|Output Formats|Text only|Text, PPT, Audio|Text only|Text + citations|
|Best For|Adaptive agents, memory|Research automation|Coding & multi-agent tasks|Truthful deep web research|
|Setup Complexity|Medium|High|Medium|Very High|

---

# **Week 2 Summary**

### **1. Implementing Vosk Speech Recognition System**

**GitHub Repository:** [alphacep/vosk-api](https://github.com/alphacep/vosk-api)

**Overview:**  
Vosk is an open-source speech recognition toolkit that supports offline real-time transcription with lightweight models. It works across platforms like Python, Java, C++, and Android.

**‚úÖ Key Highlights:**

- Successfully tested ASR (Automatic Speech Recognition) with pre-trained English and Hindi models.
    
- Integrated microphone streaming for live transcription.
    
- Explored JSON-based structured output (text + word timestamps).
    

**‚ö†Ô∏è Challenges:**

- Accuracy drops in noisy environments.
    
- Limited support for domain-specific terminology unless custom models are trained.
    

---

### **2. Exploring SpeechBrain (for Speech Models)**

**GitHub Repository:** [speechbrain/speechbrain](https://github.com/speechbrain/speechbrain)

**Overview:**  
SpeechBrain is an open-source toolkit based on PyTorch for speech-related tasks like ASR, speaker recognition, speech enhancement, and emotion detection.

**‚úÖ Work Done / Features:**

- Reviewed ASR pipelines using Transformer and CRDNN-based models.
    
- Understood training vs. inference workflows.
    
- Identified ability to fine-tune custom datasets for specific domains.
    

**‚ö†Ô∏è Challenges:**

- Requires GPU for training larger models.
    
- More complex configuration compared to Vosk.
    

---

### **3. Implementing Wav2Vec 2.0 (Meta AI)**

**GitHub (Models via HuggingFace):** `facebook/wav2vec2-base-960h`

**Overview:**  
Wav2Vec 2.0 is a self-supervised speech model that learns from raw audio without transcriptions and achieves high accuracy after fine-tuning.

**‚úÖ Achievements:**

- Loaded pre-trained Wav2Vec2 using HuggingFace and transcribed sample .wav files.
    
- Explored fine-tuning pipeline for Indian languages using Common Voice dataset.
    

**‚ö†Ô∏è Challenges:**

- Requires GPU for fine-tuning.
    
- Larger models (Wav2Vec2-Large) consume high VRAM.
    

---

### **4. Testing OpenAI Whisper (Automatic Speech Recognition)**

**Tool Used:** `openai-whisper` & Ollama Whisper models

**Overview:**  
Whisper is a multilingual ASR model by OpenAI with high accuracy in transcription and translation.

**‚úÖ Achievements:**

- Tested small, medium, and tiny models.
    
- Transcribed English + Hindi audio.
    
- Used `whisper.cpp` and `Ollama dimazv/whisper-tiny` for on-device inference.
    

**‚ö†Ô∏è Challenges:**

- Larger models need GPU/Apple Silicon for real-time transcription.
    
- Tiny models are fast but less accurate.
    

---

### **5. Testing Ollama‚Äôs Whisper Model (`dimazv/whisper-tiny`)**

**Overview:**  
Used lightweight Whisper variant through Ollama for local transcription.

**‚úÖ Observations:**

- Extremely fast inference on CPU.
    
- Works offline and supports multi-language inputs.
    
- Integrated easily with Python API + local LLMs (Gemma, GPT-OSS).
    

**‚ö†Ô∏è Limitations:**

- Lower accuracy on noisy speech or mixed languages.
    
- No speaker diarization.
    

---

|Feature / Model|**OpenAI Whisper**|**Wav2Vec 2.0 (Meta AI)**|**Vosk**|**SpeechBrain**|
|---|---|---|---|---|
|**Type**|End-to-end encoder‚Äìdecoder ASR|Self-supervised speech model|Offline speech recognition toolkit|Modular speech processing framework|
|**Offline Capability**|‚úÖ Yes (via whisper.cpp / Ollama)|‚úÖ Yes (HuggingFace, fine-tuned models)|‚úÖ Yes|‚úÖ Yes|
|**Languages Supported**|~99+ languages (multilingual)|Mostly English (few multilingual models)|Depends on model (20+ languages)|Multi-language (via custom/pretrained models)|
|**Accuracy**|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Very High)|‚≠ê‚≠ê‚≠ê‚≠ê (High after fine-tuning)|‚≠ê‚≠ê‚≠ê (Moderate, depends on model)|‚≠ê‚≠ê‚≠ê‚≠ê (High with right configuration)|
|**Real-time Performance**|Medium (small models only)|Moderate / GPU-dependent|‚úÖ Excellent (low latency)|Medium (depends on model size)|
|**Hardware Requirements**|High (GPU recommended for large models)|High for training, moderate for inference|Low (CPU-friendly)|Medium to High (GPU preferred)|
|**Model Size**|39MB (tiny) ‚Üí 1.5GB (large)|95MB (base) ‚Üí 1GB+ (large)|Small (50‚Äì200MB)|Varies (50MB to 1GB)|
|**Fine-tuning Support**|‚ùå Not open for fine-tuning (only inference)|‚úÖ Yes (self-supervised + supervised)|‚ö†Ô∏è Limited‚Äîrequires Kaldi training|‚úÖ Yes (supported with custom datasets)|
|**Best Use Cases**|Multilingual transcription, translation, noisy audio|Domain-specific ASR, research, custom dataset training|Lightweight offline voice recognition (IoT, embedded)|Research, speaker ID, ASR, emotion, diarization|
|**Ease of Use**|‚úÖ Easy (pip install whisper)|Medium (requires HuggingFace + PyTorch)|‚úÖ Very Easy (lightweight API)|Medium (needs config & scripting)|
|**Open-source License**|MIT (but weights from OpenAI)|Apache 2.0 (via Facebook / HuggingFace)|Apache 2.0|Apache 2.0|
|**Unique Features**|Translation + transcription + timestamping|Self-supervised learning on raw audio|Lightweight, runs on Raspberry Pi, Android|Complete speech ecosystem (ASR + speaker + emotion)|

---

#### ‚úÖ **Comparision Table**
|If you need...|Use This Model|
|---|---|
|Multilingual, high accuracy speech-to-text|**Whisper**|
|Custom ASR training on your dataset|**Wav2Vec 2.0**|
|Lightweight offline speech recognition|**Vosk**|
|Full speech AI pipeline (ASR + speaker ID + emotion)|**SpeechBrain**|

### **6. Implementing Self-RAG (Text-based Retrieval-Augmented Generation)**

**Overview:**  
Self-RAG is a technique where the LLM retrieves external knowledge dynamically and generates responses using context-aware reasoning.

**‚úÖ Progress:**

- Built a pipeline using local vector databases (FAISS/Chroma).
    
- Tokenized and embedded documents using sentence-transformers.
    
- Integrated retrieval + response generation into a single LLM pipeline.
    

---

### **7. Explored **SIM-RAG vs Self-RAG**

|Feature|Self-RAG|SIM-RAG (Simple RAG)|
|---|---|---|
|Retrieval Type|Dynamic + Self-evaluation|Static retrieval from vector DB|
|Feedback Loop|Model verifies and improves answer|No self-correction|
|Accuracy|Higher due to iterative refinement|Basic, sometimes shallow responses|
|Complexity|Medium to High|Low (simpler to implement)|

---

### **8. Integrated Text + Image-based Self-RAG using Ollama & GPT-OSS**

**Models Used:**

- `ollama gemma3:1b`
    
- `gpt-oss:20b-cloud`
    

**‚úÖ Achievements:**

- Processed **images using Vision API + embeddings**.
    
- Converted image features into vector representations for retrieval.
    
- Combined **text & image context** before query generation.
    
- Successfully tested Image Question Answering (IQA) + Document-based RAG.
    

**‚ö†Ô∏è Challenges:**

- Image embedding models increase memory usage.
    
- Slow inference on CPU-only systems.
    
- Needs optimized prompt design for multimodal reasoning.

---

# **Week 3 Summary**

This week focused on exploring and comparing multiple Python libraries for extracting data from PDF files‚Äîboth text-based and scanned (image-based) documents.

---

### **1. PyMuPDF (Fitz)**

**Overview:**  
PyMuPDF (Fitz) is a fast and lightweight library that enables access to PDF, XPS, EPUB, and image documents. It supports text extraction, image extraction, metadata parsing, and annotations.

**‚úÖ Key Features:**

- Extracts **structured text with coordinates** (`page.get_text("dict")` / `"blocks"`).
    
- Can extract **images, fonts, metadata, hyperlinks, and annotations**.
    
- Supports **highlighting, editing, and rendering pages as images**.
    

**‚ö†Ô∏è Limitations:**

- Not ideal for handwritten/scanned PDFs (requires OCR integration).
    
- Outputs raw coordinates‚Äîrequires manual formatting for clean datasets.
    

---

### **2. PyPDF / PyPDF2**

**Overview:**  
PyPDF (formerly PyPDF2) is a pure-Python library mainly used for PDF manipulation‚Äîmerging, splitting, rotating, watermarking, and basic text extraction.

**‚úÖ Pros:**

- Lightweight and easy to use for **splitting, merging, and metadata extraction**.
    
- Good for **searchable (digitally generated) PDFs**.
    
- No external dependencies.
    

**‚ö†Ô∏è Cons:**

- **Poor at extracting layout-aware structured text**.
    
- Does **not support OCR or scanned PDFs**.
    
- No support for extracting images or table formats.
    

---

### **3. Pytesseract (OCR using Tesseract + PDF as Images)**

**Overview:**  
Pytesseract is a Python wrapper for the Tesseract OCR engine. It extracts text from **scanned or image-based PDFs** by converting pages into images.

**‚úÖ Pros:**

- Works well for **non-searchable PDFs and scanned documents**.
    
- Supports multiple languages via Tesseract language packs.
    
- Can extract text from **images, tables, handwritten text (partially)**.
    

**‚ö†Ô∏è Cons:**

- Requires conversion of PDF to images (`pdf2image` or PyMuPDF rendering).
    
- **Accuracy depends on scan quality / noise**.
    
- Slower than text-based parsers and lacks layout retention.
    

---

### **4. PDFPlumber**

**Overview:**  
PDFPlumber is a powerful tool for parsing **structured elements like tables, lines, words, and characters** from text-based PDFs.

**‚úÖ Pros:**

- Accurate **table extraction** with layout retention.
    
- Provides coordinates for **characters, words, lines, and shapes**.
    
- Ideal for **invoice processing, forms, academic papers, structured layouts**.
    

**‚ö†Ô∏è Cons:**

- Works only with **text-based PDFs** (not scanned).
    
- Slower for large PDFs compared to PyMuPDF.
    

---

### **5. PDFMiner**

**Overview:**  
PDFMiner is a low-level PDF parsing library focused entirely on text extraction and layout analysis.

**‚úÖ Pros:**

- Extracts text with **font size, style, position, and layout**.
    
- Good for extracting **page structure and hierarchies**.
    
- Useful for PDF-to-HTML conversion.
    

**‚ö†Ô∏è Cons:**

- **Slower and more complex API** compared to PDFPlumber or PyMuPDF.
    
- Does not support images extraction.
    
- Struggles with scanned/non-searchable PDFs.
    

---

### ‚úÖ **Comparison Table**

|Library|Extracts Text|Extracts Tables|OCR Support|Layout Aware|Scanned PDF Support|
|---|---|---|---|---|---|
|PyMuPDF|‚úÖ Yes|‚ùå Limited|‚ùå No|‚úÖ Yes|‚ùå No (unless combined with OCR)|
|PyPDF|‚úÖ Yes|‚ùå No|‚ùå No|‚ùå Minimal|‚ùå No|
|Pytesseract|‚úÖ Yes (via OCR)|‚ùå Limited|‚úÖ Yes|‚ùå No (plain text)|‚úÖ Yes|
|PDFPlumber|‚úÖ Yes|‚úÖ Yes (best)|‚ùå No|‚úÖ Yes|‚ùå No|
|PDFMiner|‚úÖ Yes|‚ùå Limited|‚ùå No|‚úÖ Yes|‚ùå No|

---

### **6 FAISS (Facebook AI Similarity Search)**

**Overview:**  
FAISS is a **fast, lightweight, local vector search library** used to store high-dimensional embeddings and retrieve similar text passages using cosine similarity or inner product.

**‚úÖ What was implemented:**

- Extracted PDF text ‚Üí Created text chunks (300‚Äì500 words).
    
- Converted chunks into embeddings using Sentence Transformers (`all-MiniLM-L6-v2`).
    
- Stored embeddings in **FAISS IndexFlatL2 / IndexHNSW**.
    
- Implemented top-k semantic search for question answering.
    

**‚ö° Pros:**

- Very fast similarity search.
    
- Works offline, no server required.
    
- Good for prototyping RAG locally.
    

**‚ö†Ô∏è Limitations:**

- No built-in metadata filtering (like author, page number).
    
- No persistent storage unless manually saved as `.index` files.
    
- Not ideal for multi-user or production environments.
    

---

### **7 Qdrant (Cloud & Local Vector Database)**

**Overview:**  
Qdrant is a **production-ready vector database** with support for filtering, metadata storage, REST API, and scalable deployment.

**‚úÖ What was implemented:**

- Deployed Qdrant locally via Docker.
    
- Created a collection called `"pdf_documents"`.
    
- Stored text embeddings + metadata (`{"source": "manual.pdf", "page": 12}`).
    
- Ran vector search with **semantic similarity + metadata filters**.
    

**‚ö° Pros:**

- Supports **filters + metadata querying** (`page`, `file_name`, `section`).
    
- Persistent storage (unlike FAISS).
    
- Cloud or on-prem deployment possible.
    
- Works with Python, REST API, or FastAPI.
    

**‚ö†Ô∏è Limitations:**

- Slightly slower than FAISS for pure vector search.
    
- Requires running a server (Docker or cloud).
    
- More setup compared to FAISS.

### ‚úÖ **Comparison Table**
|Feature|**FAISS**|**Qdrant**|
|---|---|---|
|Deployment|Local / In-memory|Local or Cloud server-based|
|Persistence|Manual (.index file)|Automatic database storage|
|Metadata|‚ùå Not supported|‚úÖ Yes (JSON payloads)|
|Filtering|‚ùå No|‚úÖ Yes (filter by fields)|
|Speed|üöÄ Very Fast|‚ö° Fast, but slightly slower|
|Best For|Prototyping, offline RAG|Production-ready RAG systems|

### **8. Implementing Rerankers (Re-Ranking in RAG Pipelines)**

In a standard RAG (Retrieval-Augmented Generation) system:  
**User Query ‚Üí Vector Search (FAISS/Qdrant) ‚Üí Top-k Documents ‚Üí LLM ‚Üí Answer**

However, vector search alone may return **semantically similar but contextually irrelevant results**.  
To improve accuracy, a **Reranker/ReRanker Model** is added after retrieval to re-score and reorder the search results based on relevance to the user‚Äôs query.

This step is called **Re-ranking** or **Reranker Stage**, and models like **Cross-Encoders (BERT, MiniLM, bge-reranker, colbert)** are commonly used.

|Problem with Basic Vector Search|How Reranker Solves It|
|---|---|
|Returns top-k results based only on cosine similarity.|Ranks based on deeper query-passage understanding.|
|Important context may appear in lower results (rank 10‚Äì20).|Reranker rescans results and brings the best to the top.|
|Vector embeddings are shallow (bag of semantics).|Rerankers use cross-attention (query + document together).|
|You risk hallucination if wrong context is used.|Reduces hallucination by giving LLM the **most accurate context**.|

#### **Types of Reranker Models**
|Type|Description|Example Models|
|---|---|---|
|**Cross-Encoders**|Encode _(query + document)_ together ‚Üí score relevance.|`bge-reranker-large`, `ms-marco MiniLM`, `ColBERT`, `MonoT5`|
|**Bi-Encoders (simple embedding)**|Encode separately ‚Üí only embedding similarity.|`sentence-transformers`, `BERT base`|
|**Hybrid Reranker**|Combine vector score + lexical score + cross-encoding.|BM25 + FAISS + Reranker|

#### **Limitations** ‚ö†Ô∏è 
|Limitation|Description|
|---|---|
|‚ùå Slower than embedding-only search|Because each query+doc pair is re-processed through transformer|
|‚ùå Requires GPU for large models (`bge-large`, `monoT5`)|Otherwise inference is slow|
|‚ùå Cannot handle large batch sizes unless optimized|Memory management needed|
|‚ùå Works only after first retrieval step|It doesn‚Äôt search, only refines|

### **9. Explored Rex-Omni: Detect Anything via Next Point Prediction**

**Website:** [rex-omni.github.io](http://rex-omni.github.io/)
**Paper:** [Rex-Omni Research Paper][https://arxiv.org/pdf/2510.12798]
**GitHub Repository:** [IDEA-Research/Rex-Omni](https://github.com/IDEA-Research/Rex-Omni)

**Rex-Omni** is a new **foundation model for universal object detection**, capable of detecting _anything_ (objects, parts, shapes, landmarks) using a **Next Point Prediction (NPP)** approach.  
Unlike traditional detectors (YOLO, Mask R-CNN) that rely on bounding boxes or segmentation masks, Rex-Omni predicts object structures as **sequences of points**, enabling high flexibility for:

- Object detection
    
- Keypoint detection
    
- Instance & semantic segmentation
    
- Open-vocabulary detection
    
- Referring (text-guided) object localization

### üöÄ **Key Features**

|Capability|Description|
|---|---|
|**Next Point Prediction (NPP)**|Objects are represented as sequential points instead of bounding boxes.|
|**Detect Anything**|Works for objects, parts, contours, landmarks, and object boundaries.|
|**Unified Model**|A single model supports segmentation, detection, visual grounding, and keypoints.|
|**Open-Vocabulary & Multimodal**|Supports text prompts like ‚Äúfind the wrench near the valve‚Äù.|
|**Strong Performance**|Competes with state-of-the-art on COCO, LVIS, and keypoint datasets.|

---

### üõ†Ô∏è **Architecture Summary**

Rex-Omni consists of three main components:

|Component|Function|
|---|---|
|**Vision Encoder (ViT / Swin Transformer)**|Extracts visual features from images.|
|**Point Sequence Generator (NPP Model)**|Predicts object boundaries or keypoints sequentially.|
|**Multimodal Text Encoder (for grounding)**|Accepts natural language prompts for object-specific search.|

---

### ‚úÖ **Pros**

- **Unified detection framework** ‚Üí replaces multiple task-specific models.
    
- **More flexible than bounding box detectors** ‚Üí works on contours, thin objects, or irregular shapes.
    
- **Enables vision-language tasks** ‚Üí can detect objects using natural language.
    
- **Open-source & actively maintained** with pretrained models and demo notebook.
    
- **Great for robotics, AR/VR, industrial inspection, and smart documentation systems**.
    

---

### ‚ö†Ô∏è **Cons / Limitations**

- **High computational cost** (requires GPU for real-time usage).
    
- **Complex architecture** ‚Üí harder to deploy on edge devices compared to YOLO.
    
- **Still research-stage** with fewer real-world industry integrations.
    
- **Relies heavily on high-quality image input** ‚Üí weak in low-light or blurry environments.
    

---

### üí° **Possible Use in IETM + AR/VR Project**

|Use Case|Application Idea|
|---|---|
|**Maintenance Manuals (IETM)**|Detect machinery parts in real-time and overlay repair steps.|
|**AR/VR Assistance**|In XR headsets, highlight exact screws, wires, or machine parts to work on.|
|**Interactive Technical Diagrams**|Automatically convert mechanical diagrams into clickable 3D components.|
|**Object Tracking in Training Simulations**|Guide trainees by detecting tools and ensuring correct workflow sequence.|

### üìä **Comparison: Rex-Omni vs YOLO vs SAM vs DETR**

|Feature / Model|**Rex-Omni**|**YOLO (v8/v9)**|**DETR (Transformers)**|**SAM (Segment Anything Model)**|
|---|---|---|---|---|
|**Primary Task**|Detect anything using **Next Point Prediction (NPP)**|Fast real-time **bounding box detection**|Transformer-based **object detection & segmentation**|**Zero-shot segmentation** (given click/box/prompt)|
|**Output Type**|Sequence of **points (contours, keypoints, masks)**|**Bounding boxes + class labels**|**Bounding boxes + masks** via attention|**Pixel-accurate mask**|
|**Supports Segmentation?**|‚úÖ Yes (via point sequences)|‚ö†Ô∏è Only in YOLO-Seg versions|‚úÖ Yes (DETR + Mask-DETR)|‚úÖ Yes|
|**Open-Vocabulary (Text Prompt Detection)**|‚úÖ Yes (detect via language prompts)|‚ùå No|‚ö†Ô∏è Limited (requires OV-DETR)|‚úÖ Yes (with prompts)|
|**Handles Irregular Shapes (contours)**|‚úÖ Excellent|‚ùå Bounding box only|‚ö†Ô∏è Moderate|‚úÖ Yes|
|**Detects Object Parts / Keypoints**|‚úÖ Yes (landmarks, edges, contours)|‚ö†Ô∏è Limited (only with YOLO-Pose variants)|‚úÖ Yes (DETR-Pose)|‚ùå Not intended for keypoints|
|**Real-time Speed**|‚ùå Slower (heavy vision transformer + NPP)|‚úÖ Very Fast (30‚Äì120 FPS on GPU)|‚ö†Ô∏è Moderate (~10‚Äì20 FPS)|‚ö†Ô∏è Slow to Moderate|
|**Training Complexity**|‚ö†Ô∏è High (sequential point training)|‚úÖ Low (easy to train/dataset ready)|‚ö†Ô∏è Medium-High|‚ùå No training (foundational model)|
|**Works on Scanned Manuals / CAD / Technical Docs?**|‚úÖ Yes (point-based = precise shapes)|‚ö†Ô∏è Only coarse detection|‚úÖ Possible with fine-tuning|‚úÖ Excellent with high-res input|
|**Best Use Cases**|AR manuals, robotics, part detection, scientific/medical tasks|CCTV, autonomous vehicles, face/person detection|Research-grade detection, segmentation, high accuracy|Object segmentation, annotation tools, editing|
|**Model Size**|‚ö†Ô∏è Large (ViTs, multi-task)|‚úÖ Small to medium (10‚Äì200MB)|‚ö†Ô∏è Medium to large|‚ö†Ô∏è Very large (2‚Äì4GB)|
|**Open Source**|‚úÖ Yes|‚úÖ Yes|‚úÖ Yes|‚úÖ Yes (Meta AI)|

---

### ‚úÖ **Summary ‚Äì When to Use What?**

|If you want to‚Ä¶|Use This|
|---|---|
|Detect **irregular-shaped objects, parts, or contours**|‚úÖ **Rex-Omni**|
|Do **fast real-time object detection (CCTV, drones, defense systems)**|‚úÖ **YOLO**|
|Do **high-precision, end-to-end detection + segmentation using transformers**|‚úÖ **DETR**|
|**Extract object masks with a single click or prompt**|‚úÖ **SAM (Segment Anything)**|
|Build **IETM/AR VR part-aware manuals or machine part intelligence**|‚úÖ **Rex-Omni** or SAM + Rex-Omni combo|

## **Week 4 Summary**

### **RAG with vision encoder with relevant inputs**

#### ‚úÖ **1) Multimodal RAG with Vision Encoders**

##### **üîπ Overview**

This week, work progressed from traditional text-only RAG to **Multimodal Retrieval-Augmented Generation (MM-RAG)**‚Äîwhere models can retrieve and reason over both **visual (images, diagrams, scanned PDFs)** and **textual knowledge**.  
This is critical for applications like **IETM manuals**, **AR-based equipment maintenance**, and **technical diagrams understanding**, where information is often visual.

#### ‚úÖ **2) Implemented Multimodal RAG**

##### **‚úî Implementation Steps Completed:**

|Stage|Description|
|---|---|
|**PDF/Image Parsing**|Used PyMuPDF + OCR for diagrams, technical images.|
|**Embedding Generation**|Used **CLIP** for image & text embeddings, stored in **FAISS/Qdrant**.|
|**Cross-modal Retrieval**|Text‚ÜíImage and Image‚ÜíText retrieval enabled.|
|**Reranking Added**|Used **Cross-Encoder / CLIP-Reranker** to reorder Top-K results for accuracy.|
|**Final QA Generation**|Used a vision-language model (e.g., LLaVA / BLIP-2) to answer using Top-3 most relevant results.|

---

#### ‚úÖ **3) What is a Reranker & Why It Matters?**

##### **üõë Problem in Basic RAG (without reranker):**

- FAISS/Qdrant ranks results using vector cosine similarity only.
    
- Retrieved results may be **semantically close but contextually irrelevant**.
    
- Leads to **hallucination or incorrect answers** by the LLM.
    

##### **‚úÖ Solution: Reranking Layer**

- Re-scores the retrieved documents **using a more accurate model** that reads both **query + document together**.
    
- Example models:
    
    - **bge-reranker-large**
        
    - **CrossEncoder(ms-marco-MiniLM)**
        
    - **CLIP-based cross-attention reranker for image matching**

#### ‚úÖ **4) Multimodal RAG Approaches (with Reranking Perspective)**

|Approach|How Retrieval Works|Supports Reranking?|Best For|
|---|---|---|---|
|**CLIP Dual Encoder**|Image ‚Üî Text embeddings|‚úÖ Yes (CLIP-Score or Cross-Encoder)|Fast visual search|
|**BLIP-2 / InstructBLIP**|Q-Former encodes image tokens to LLM|‚úÖ In-built attention|Instruction-aware image reasoning|
|**Florence-2**|OCR + grounding + image caption embedding|‚úÖ + Region reranking|Diagrams, documents, manuals|
|**Hierarchical RAG (Wiki-LLaVA)**|Stage 1: Caption retrieval; Stage 2: image-level rerank|‚úÖ Yes|Large datasets + cost-efficient|
|**Self-RAG with Reflection**|Model critiques retrieval and requests more if needed|‚úÖ Self-reranking|Autonomous agents|

#### ‚úÖ **5) Comparison ‚Äî Reranker Impact on Multimodal RAG**
|Feature|No Reranker|With Reranker|
|---|---|---|
|Accuracy|‚ùå Medium|‚úÖ High|
|Hallucinations|‚ùå Frequent|‚úÖ Reduced|
|Visual-Text Matching|Basic cosine score|‚úÖ Cross-attention relevance|
|Industrial Use Readiness|‚ùå Prototype only|‚úÖ Deployment-ready|
|Example Use Case|‚ÄúFind any pump image‚Äù|‚ÄúFind correct pump valve diagram on page 4 of manual‚Äù|

#### Sources of my reading

**Research Paper:** [mR<sup>2</sup>AG](https://arxiv.org/pdf/2411.15041)
Point to Note: please read `Supplementary Material` 

**Blog:** [RAG + llama3](https://zilliz.com/blog/multimodal-RAG-with-CLIP-Llama3-and-milvus)

**Technical Blog of: **[Nvidia](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/)

**Research Paper:** [WavRAG](https://arxiv.org/pdf/2502.14727)

**Fine Tune Embeddung Model** [YouTube](https://www.youtube.com/watch?v=v28Pu7hsJ0s)